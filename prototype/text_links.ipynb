{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_text_from_pdf(file_path):\n",
    "        \"\"\"Extract text from PDF file using pdfplumber for better text extraction\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            # Use pdfplumber for better formatting preservation\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text(x_tolerance=3, y_tolerance=3)\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                    \n",
    "            # If pdfplumber didn't get good results, try PyPDF2 as fallback\n",
    "            if not text.strip():\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    for page_num in range(len(pdf_reader.pages)):\n",
    "                        text += pdf_reader.pages[page_num].extract_text() + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {str(e)}\")\n",
    "        \n",
    "        # print(text)\n",
    "        return text\n",
    "\n",
    "    \n",
    "def extract_hyperlinks_from_pdf(file_path):\n",
    "        \"\"\"Extract hyperlinks from PDF file using PyMuPDF to get both link text and URLs\"\"\"\n",
    "        hyperlinks = []\n",
    "        \n",
    "        try:\n",
    "            doc = fitz.open(file_path)\n",
    "            for page_num, page in enumerate(doc):\n",
    "                link_list = page.get_links()\n",
    "                for link in link_list:\n",
    "                    if link.get(\"uri\"):\n",
    "                        # Get the rectangle containing the link\n",
    "                        rect = link.get(\"from\")\n",
    "                        \n",
    "                        # Extract the text in this rectangle if available\n",
    "                        link_text = \"\"\n",
    "                        if rect:\n",
    "                            words = page.get_text(\"words\", clip=rect)\n",
    "                            link_text = \" \".join([word[4] for word in words]) if words else \"\"\n",
    "                        \n",
    "                        hyperlinks.append({\n",
    "                            'text': link_text,\n",
    "                            'url': link.get(\"uri\")\n",
    "                        })\n",
    "            doc.close()\n",
    "        except Exception as e:\n",
    "            print(f\"PyMuPDF extraction error: {str(e)}\")\n",
    "        \n",
    "        return hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'deopurkar.a@northeastern.edu', 'url': 'mailto:deopurkar.a@northeastern.edu'}, {'text': 'LinkedIn', 'url': 'https://www.linkedin.com/in/ankit-deopurkar/'}]\n"
     ]
    }
   ],
   "source": [
    "print(extract_hyperlinks_from_pdf(\"ad.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tanmay Pawar\\n(857) 339-8799 | pawar.ta@northeastern.edu | Boston, MA, 02120 | LinkedIn | GitHub\\nEDUCATION\\nNortheastern University Boston, MA\\nMaster of Science in Information Systems Expected May 2026\\nRelated Coursework: Data Science, Big Data Intelligence, Database Management GPA: 4\\nPune University Pune, India\\nBachelor of Engineering in Computer Engineering Mar 2018 - Jun 2022\\nRelated Coursework: Computer Organization and Architecture, Machine Learning, DBMS, Cloud\\nSKILLS\\nProgramming Languages: Python, C#, Java, HTML, CSS, JavaScript\\nDatabases & Tools: Oracle SQL, MSSQL, Snowflake, dbt, Git, Apache Airflow\\nCloud Platforms: AWS, GCP, Azure\\nLibraries & Frameworks: Pandas, NumPy, scikit-learn, ReactJS, .NET, Streamlit, LangChain\\nCertification: AZ-900(Azure Fundamentals)\\nWORK EXPERIENCE\\nPersistent Systems Ltd | Software Engineer (Jul 2022 - Jul 2024) Pune, India\\n• Cleaned and managed datasets using Python (Pandas, NumPy) and SQL, improving data quality\\n• Conducted data analysis using Python libraries to identify trends and patterns, supporting model fine-tuning and improving\\npredictive accuracy\\n• Built interactive dashboards with Tableau and Python that provided real-time analytics and empowered business teams to\\nmake informed decisions\\n• Implemented CI/CD pipelines using GitHub Actions, automating testing, building, and deployment processes for data\\nengineering projects, resulting in a 50% reduction in deployment time and improved code quality\\n• Built APIs within the .NET core and developed SQL stored procedures to optimize data retrieval for new features, cutting\\nquery times by 30% and improving system integration\\n• Spearheaded deployment through Azure DevOps, accelerating release cycles and ensuring compliance with company\\nstandards\\nPersistent Systems Ltd | Intern (Feb 2022 - Jul 2022) Pune, India\\n• Assisted in creating and managing dynamic, scalable ReactJS components, contributing to a smoother user experience\\nacross multiple platforms and helping reduce load times by 20%\\n• Supported the design and development of RESTful APIs using .NET Core, which improved data retrieval efficiency\\n• Collaborated closely with cross-functional teams to ensure seamless integration between front-end and back-end systems\\n• Wrote SQL queries to extract data from various source tables and files, facilitating backend testing and improving data\\naccuracy\\nPROJECTS\\nFindata | Python, Snowflake, Apache Airflow, AWS, dbt, Streamlit, FastAPI, Docker, GCP\\n• Built an end-to-end financial data pipeline for SEC Financial Statement Data Sets using Snowflake, Airflow, and AWS S3.\\nDesigned efficient storage (Raw Staging, JSON, Denormalized Fact Tables) with dbt transformations and deployed a full-\\nstack Streamlit-FastAPI application on GCP using Docker\\nPytract | PyPDF2, Pdfplumber, BeautifulSoup, Docling, AWS S3, FastAPI, Streamlit\\n• Built a prototype for document extraction, standardization, and storage using Python and AWS S3. Exposed data via\\nFastAPI and a Streamlit UI to evaluate scalable processing workflows\\nMediSense | AWS S3, LangGraph, RAG, Streamlit, GCP\\n• Developed MediSense, a medical data pipeline using BeautifulSoup, AWS S3, and LangChain for efficient web data\\nextraction and AI-powered retrieval (RAG, Gemini API). Built an interactive Streamlit UI and deployed on GCP for\\nscalability\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_extract_text_from_pdf(\"tanmay.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
