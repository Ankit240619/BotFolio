{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--output OUTPUT] input_file\n",
      "ipykernel_launcher.py: error: the following arguments are required: input_file\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\BigDataIntelligence\\test\\test\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] tanmay.pdf data.jso\n",
      "ipykernel_launcher.py: error: the following arguments are required: tanmay.pdf, data.jso\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\BigDataIntelligence\\test\\test\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "\n",
    "class ResumeParser:\n",
    "    def __init__(self):\n",
    "        self.extracted_data = {}\n",
    "        \n",
    "        # Regex patterns for extraction\n",
    "        self.email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        self.phone_pattern = r'(\\+\\d{1,3}[-\\.\\s]??)?\\(?\\d{3}\\)?[-\\.\\s]?\\d{3}[-\\.\\s]?\\d{4}'\n",
    "        self.linkedin_pattern = r'linkedin\\.com/in/[a-zA-Z0-9_-]+|LinkedIn'\n",
    "        self.github_pattern = r'github\\.com/[a-zA-Z0-9_-]+|GitHub'\n",
    "        \n",
    "        # Common section headers in resumes\n",
    "        self.section_headers = {\n",
    "            'education': ['education', 'academic background', 'academic history', 'educational background'],\n",
    "            'experience': ['experience', 'work experience', 'employment history', 'work history', 'professional experience'],\n",
    "            'skills': ['skills', 'technical skills', 'core competencies', 'key skills', 'areas of expertise'],\n",
    "            'projects': ['projects', 'personal projects', 'academic projects', 'key projects'],\n",
    "            'certifications': ['certifications', 'certificates', 'professional certifications', 'credentials'],\n",
    "            'languages': ['languages', 'language proficiency', 'spoken languages']\n",
    "        }\n",
    "\n",
    "    def parse_resume(self, file_path):\n",
    "        \"\"\"Parse resume file (PDF only)\"\"\"\n",
    "        _, file_extension = os.path.splitext(file_path)\n",
    "        file_extension = file_extension.lower()\n",
    "        \n",
    "        if file_extension != '.pdf':\n",
    "            print(f\"Only PDF format is supported. Provided file: {file_extension}\")\n",
    "            return None\n",
    "        \n",
    "        text = self._extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            print(\"Failed to extract text from PDF.\")\n",
    "            return None\n",
    "        \n",
    "        # Extract information from the text\n",
    "        self._extract_information(text)\n",
    "        return self.extracted_data\n",
    "    \n",
    "    def _extract_text_from_pdf(self, file_path):\n",
    "        \"\"\"Extract text from PDF file using pdfplumber for better text extraction\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            # Use pdfplumber for better formatting preservation\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text(x_tolerance=3, y_tolerance=3)\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                    \n",
    "            # If pdfplumber didn't get good results, try PyPDF2 as fallback\n",
    "            if not text.strip():\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    for page_num in range(len(pdf_reader.pages)):\n",
    "                        text += pdf_reader.pages[page_num].extract_text() + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {str(e)}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _extract_information(self, text):\n",
    "        \"\"\"Extract various information from text\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        # Extract personal information\n",
    "        personal_info = self._extract_personal_info(text)\n",
    "        if personal_info:\n",
    "            self.extracted_data[\"personal_info\"] = personal_info\n",
    "        \n",
    "        # First locate all section headings and their line numbers\n",
    "        section_bounds = self._identify_sections(lines)\n",
    "        \n",
    "        # Extract contents of each section\n",
    "        for section_name, (start_idx, end_idx) in section_bounds.items():\n",
    "            section_content = '\\n'.join(lines[start_idx+1:end_idx]).strip()\n",
    "            self._process_section(section_name, section_content)\n",
    "            \n",
    "        # Check for any sections that might have been missed using heuristics\n",
    "        self._extract_missing_sections(text)\n",
    "    \n",
    "    def _identify_sections(self, lines):\n",
    "        \"\"\"Identify sections in the resume and their line bounds\"\"\"\n",
    "        section_bounds = {}\n",
    "        section_starts = []\n",
    "        \n",
    "        # First identify all section headers and their line numbers\n",
    "        for idx, line in enumerate(lines):\n",
    "            line_clean = line.strip().lower()\n",
    "            \n",
    "            # Skip empty lines\n",
    "            if not line_clean:\n",
    "                continue\n",
    "            \n",
    "            # Check if this line matches any known section header\n",
    "            found_section = None\n",
    "            for section, keywords in self.section_headers.items():\n",
    "                # Try exact match\n",
    "                if line_clean in keywords:\n",
    "                    found_section = section\n",
    "                    break\n",
    "                \n",
    "                # Try partial match - the line contains the keyword\n",
    "                for keyword in keywords:\n",
    "                    if keyword in line_clean and len(line_clean) < 30:\n",
    "                        found_section = section\n",
    "                        break\n",
    "                \n",
    "                if found_section:\n",
    "                    break\n",
    "            \n",
    "            # Special case for sections with unique formatting\n",
    "            if found_section is None and line_clean == 'work experience':\n",
    "                found_section = 'experience'\n",
    "            elif found_section is None and line_clean == 'projects':\n",
    "                found_section = 'projects'\n",
    "            \n",
    "            if found_section:\n",
    "                section_starts.append((idx, found_section))\n",
    "        \n",
    "        # Determine section bounds (start and end lines)\n",
    "        for i, (start_idx, section_name) in enumerate(section_starts):\n",
    "            if i < len(section_starts) - 1:\n",
    "                end_idx = section_starts[i+1][0]\n",
    "            else:\n",
    "                end_idx = len(lines)\n",
    "            \n",
    "            section_bounds[section_name] = (start_idx, end_idx)\n",
    "        \n",
    "        return section_bounds\n",
    "    \n",
    "    def _extract_personal_info(self, text):\n",
    "        \"\"\"Extract personal information like name, email, phone\"\"\"\n",
    "        personal_info = {}\n",
    "        \n",
    "        # Extract email\n",
    "        emails = re.findall(self.email_pattern, text)\n",
    "        if emails:\n",
    "            personal_info[\"email\"] = emails[0]\n",
    "        \n",
    "        # Extract phone\n",
    "        phones = re.findall(self.phone_pattern, text)\n",
    "        if phones:\n",
    "            personal_info[\"phone\"] = phones[0]\n",
    "        \n",
    "        # Extract LinkedIn\n",
    "        linkedin = re.findall(self.linkedin_pattern, text)\n",
    "        if linkedin:\n",
    "            personal_info[\"linkedin\"] = linkedin[0]\n",
    "            \n",
    "        # Extract GitHub\n",
    "        github = re.findall(self.github_pattern, text)\n",
    "        if github:\n",
    "            personal_info[\"github\"] = github[0]\n",
    "        \n",
    "        # Try to extract name (this is more challenging)\n",
    "        # Assuming name might be at the beginning of the resume\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines[:3]:  # Usually the first line in a resume\n",
    "            line = line.strip()\n",
    "            if line and 1 <= len(line.split()) <= 4:  # Most names are 1-4 words\n",
    "                # Check if this line doesn't contain common headers or contact info\n",
    "                if not any(word in line.lower() for word in ['resume', 'cv', 'curriculum', 'vitae', 'email', 'phone', '@']):\n",
    "                    personal_info[\"name\"] = line\n",
    "                    break\n",
    "                    \n",
    "        # Try to extract location/address\n",
    "        location_pattern = r'Boston, MA|Pune, India'\n",
    "        location_match = re.search(location_pattern, text)\n",
    "        if location_match:\n",
    "            personal_info[\"location\"] = location_match.group(0).strip()\n",
    "            \n",
    "        return personal_info\n",
    "    \n",
    "    def _process_section(self, section_name, section_text):\n",
    "        \"\"\"Process different sections of the resume based on section name\"\"\"\n",
    "        if section_name == \"education\":\n",
    "            self.extracted_data[\"education\"] = self._extract_education(section_text)\n",
    "        elif section_name == \"experience\":\n",
    "            self.extracted_data[\"work_experience\"] = self._extract_experience(section_text)\n",
    "        elif section_name == \"skills\":\n",
    "            self.extracted_data[\"skills\"] = self._extract_skills(section_text)\n",
    "        elif section_name == \"projects\":\n",
    "            self.extracted_data[\"projects\"] = self._extract_projects(section_text)\n",
    "        elif section_name == \"certifications\":\n",
    "            self.extracted_data[\"certifications\"] = self._extract_certifications(section_text)\n",
    "        elif section_name == \"languages\":\n",
    "            self.extracted_data[\"languages\"] = self._extract_languages(section_text)\n",
    "    \n",
    "    def _extract_missing_sections(self, text):\n",
    "        \"\"\"Look for sections that might have been missed in the standard extraction\"\"\"\n",
    "        # Extract certifications if they weren't found in a distinct section\n",
    "        if \"certifications\" not in self.extracted_data and \"Certification:\" in text:\n",
    "            self.extracted_data[\"certifications\"] = self._extract_certifications(text)\n",
    "            \n",
    "        # Look for skills if not already extracted\n",
    "        if \"skills\" not in self.extracted_data:\n",
    "            skill_markers = [\"Programming Languages:\", \"Databases & Tools:\", \"Cloud Platform\"]\n",
    "            for marker in skill_markers:\n",
    "                if marker in text:\n",
    "                    self.extracted_data[\"skills\"] = self._extract_skills(text)\n",
    "                    break\n",
    "    \n",
    "    def _extract_education(self, text):\n",
    "        \"\"\"Extract education information\"\"\"\n",
    "        education_entries = []\n",
    "        \n",
    "        # Try to split by institution names\n",
    "        institution_pattern = r'([\\w\\s]+University|[\\w\\s]+College|[\\w\\s]+Institute)'\n",
    "        institutions = re.findall(institution_pattern, text)\n",
    "        \n",
    "        # If no clear institutions found, try splitting by paragraphs\n",
    "        if not institutions:\n",
    "            entries = text.split('\\n\\n')\n",
    "        else:\n",
    "            # Split by institution names\n",
    "            entries = []\n",
    "            current_entry = \"\"\n",
    "            lines = text.split('\\n')\n",
    "            current_institution = None\n",
    "            \n",
    "            for line in lines:\n",
    "                inst_match = re.search(institution_pattern, line)\n",
    "                if inst_match and (not current_institution or current_institution != inst_match.group(1)):\n",
    "                    if current_entry:\n",
    "                        entries.append(current_entry)\n",
    "                    current_entry = line + \"\\n\"\n",
    "                    current_institution = inst_match.group(1)\n",
    "                else:\n",
    "                    current_entry += line + \"\\n\"\n",
    "            \n",
    "            if current_entry:\n",
    "                entries.append(current_entry)\n",
    "        \n",
    "        for entry in entries:\n",
    "            if not entry.strip():\n",
    "                continue\n",
    "                \n",
    "            edu_item = {}\n",
    "            lines = entry.split('\\n')\n",
    "            \n",
    "            # Extract institution name\n",
    "            for i, line in enumerate(lines):\n",
    "                if \"University\" in line or \"College\" in line or \"Institute\" in line:\n",
    "                    parts = line.split(',')\n",
    "                    edu_item[\"institution\"] = parts[0].strip()\n",
    "                    \n",
    "                    # Extract location if present\n",
    "                    if len(parts) > 1:\n",
    "                        edu_item[\"location\"] = parts[1].strip()\n",
    "                    break\n",
    "            \n",
    "            # Extract degree\n",
    "            degree_pattern = r'(Master of|Bachelor of|PhD in|Doctorate in|MS in|BS in)[^,\\n]+'\n",
    "            for line in lines:\n",
    "                degree_match = re.search(degree_pattern, line, re.IGNORECASE)\n",
    "                if degree_match:\n",
    "                    edu_item[\"degree\"] = degree_match.group(0).strip()\n",
    "                    break\n",
    "                elif \"Expected\" in line and \"202\" in line:\n",
    "                    parts = line.split(\"Expected\")\n",
    "                    if parts and parts[0].strip():\n",
    "                        edu_item[\"degree\"] = parts[0].strip()\n",
    "            \n",
    "            # Extract graduation date/expected date\n",
    "            for line in lines:\n",
    "                expected_match = re.search(r'Expected\\s+([\\w\\s]+\\d{4})', line)\n",
    "                if expected_match:\n",
    "                    edu_item[\"expected_graduation\"] = expected_match.group(1).strip()\n",
    "                else:\n",
    "                    date_match = re.search(r'((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4})\\s*-\\s*((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}|\\d{4}|Present)', line, re.IGNORECASE)\n",
    "                    if date_match:\n",
    "                        edu_item[\"start_date\"] = date_match.group(1).strip()\n",
    "                        edu_item[\"end_date\"] = date_match.group(2).strip()\n",
    "            \n",
    "            # Extract GPA\n",
    "            for line in lines:\n",
    "                gpa_match = re.search(r'GPA:?\\s*(\\d+(?:\\.\\d+)?)', line)\n",
    "                if gpa_match:\n",
    "                    edu_item[\"gpa\"] = gpa_match.group(1).strip()\n",
    "            \n",
    "            # Extract coursework\n",
    "            for i, line in enumerate(lines):\n",
    "                if \"Coursework:\" in line or \"coursework:\" in line or \"Related Coursework:\" in line:\n",
    "                    course_text = line.split(\":\", 1)[1].strip()\n",
    "                    courses = [course.strip() for course in re.split(r',|;', course_text) if course.strip()]\n",
    "                    if courses:\n",
    "                        edu_item[\"coursework\"] = courses\n",
    "            \n",
    "            if edu_item:\n",
    "                education_entries.append(edu_item)\n",
    "        \n",
    "        return education_entries\n",
    "    \n",
    "    def _extract_experience(self, text):\n",
    "        \"\"\"Extract work experience information\"\"\"\n",
    "        experience_entries = []\n",
    "        \n",
    "        # Split by company entries - try multiple patterns\n",
    "        entries = []\n",
    "        \n",
    "        # First try to split by company | position pattern\n",
    "        company_pattern = r'([\\w\\s]+Ltd|[\\w\\s]+Inc|[\\w\\s]+Corp|[\\w\\s]+LLC)\\s*\\|\\s*([^|\\n]+)'\n",
    "        companies = re.findall(company_pattern, text)\n",
    "        \n",
    "        if companies:\n",
    "            # Split text by these companies\n",
    "            sections = re.split(company_pattern, text)\n",
    "            \n",
    "            # Reconstruct experience entries\n",
    "            current_entry = \"\"\n",
    "            for i in range(1, len(sections), 3):  # Skip every 3rd element (matched groups)\n",
    "                if i+2 < len(sections):\n",
    "                    company = sections[i].strip()\n",
    "                    position = sections[i+1].strip()\n",
    "                    content = sections[i+2]\n",
    "                    \n",
    "                    entry = f\"{company} | {position}\\n{content}\"\n",
    "                    entries.append(entry)\n",
    "        \n",
    "        # If no entries found with first pattern, try alternative\n",
    "        if not entries:\n",
    "            # Fall back to paragraph splitting\n",
    "            entries = text.split('\\n\\n')\n",
    "            \n",
    "            # Filter entries that look like job experiences\n",
    "            entries = [entry for entry in entries if \"|\" in entry or \"•\" in entry]\n",
    "        \n",
    "        for entry in entries:\n",
    "            if not entry.strip():\n",
    "                continue\n",
    "                \n",
    "            exp_item = {}\n",
    "            lines = entry.split('\\n')\n",
    "            \n",
    "            # Extract company and role (often in format \"Company | Role\")\n",
    "            for line in lines:\n",
    "                if \"|\" in line:\n",
    "                    parts = line.split(\"|\", 1)\n",
    "                    if len(parts) == 2:\n",
    "                        exp_item[\"company\"] = parts[0].strip()\n",
    "                        \n",
    "                        # Position might include date in parentheses\n",
    "                        position_part = parts[1].strip()\n",
    "                        if \"(\" in position_part and \")\" in position_part:\n",
    "                            position, date_part = position_part.split(\"(\", 1)\n",
    "                            exp_item[\"position\"] = position.strip()\n",
    "                            \n",
    "                            # Extract dates\n",
    "                            date_part = date_part.split(\")\", 1)[0].strip()\n",
    "                            if \"-\" in date_part:\n",
    "                                start_date, end_date = date_part.split(\"-\", 1)\n",
    "                                exp_item[\"start_date\"] = start_date.strip()\n",
    "                                exp_item[\"end_date\"] = end_date.strip()\n",
    "                        else:\n",
    "                            exp_item[\"position\"] = position_part\n",
    "                    break\n",
    "            \n",
    "            # If we don't have dates yet, try to find them on the line\n",
    "            if \"start_date\" not in exp_item:\n",
    "                for line in lines:\n",
    "                    date_match = re.search(r'\\(([\\w\\s]+\\d{4})\\s*-\\s*([\\w\\s]+\\d{4}|Present)\\)', line)\n",
    "                    if date_match:\n",
    "                        exp_item[\"start_date\"] = date_match.group(1).strip()\n",
    "                        exp_item[\"end_date\"] = date_match.group(2).strip()\n",
    "                        break\n",
    "            \n",
    "            # Extract location\n",
    "            for line in lines:\n",
    "                if \"Pune, India\" in line or \"Boston, MA\" in line:\n",
    "                    exp_item[\"location\"] = re.search(r'([\\w\\s]+,\\s*[\\w\\s]+)$', line).group(1).strip()\n",
    "                    break\n",
    "            \n",
    "            # Extract responsibilities (bullet points)\n",
    "            responsibilities = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith('•'):\n",
    "                    clean_line = line.lstrip('• ').strip()\n",
    "                    if clean_line:\n",
    "                        responsibilities.append(clean_line)\n",
    "            \n",
    "            if responsibilities:\n",
    "                exp_item[\"responsibilities\"] = responsibilities\n",
    "            \n",
    "            if exp_item:\n",
    "                experience_entries.append(exp_item)\n",
    "        \n",
    "        return experience_entries\n",
    "    \n",
    "    def _extract_skills(self, text):\n",
    "        \"\"\"Extract skills information with categorization\"\"\"\n",
    "        skills_dict = {}\n",
    "        \n",
    "        # Look for skill categories common in resumes\n",
    "        skill_categories = [\n",
    "            \"Programming Languages\", \"Databases & Tools\", \"Cloud Platform\", \n",
    "            \"Libraries & Frameworks\", \"Technologies\", \"Software\"\n",
    "        ]\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        current_category = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Check if this is a category line\n",
    "            is_category = False\n",
    "            for category in skill_categories:\n",
    "                if line.startswith(category + \":\"):\n",
    "                    current_category = category\n",
    "                    skills_text = line.split(\":\", 1)[1].strip()\n",
    "                    skills = [skill.strip() for skill in skills_text.split(\",\") if skill.strip()]\n",
    "                    skills_dict[current_category] = skills\n",
    "                    is_category = True\n",
    "                    break\n",
    "                    \n",
    "            # If we have a current category but this isn't a new one, it might be skills for current category\n",
    "            if not is_category and current_category and \":\" not in line and \"•\" not in line:\n",
    "                # Looks like additional skills for current category\n",
    "                if current_category in skills_dict:\n",
    "                    skills = [skill.strip() for skill in line.split(\",\") if skill.strip()]\n",
    "                    skills_dict[current_category].extend(skills)\n",
    "                    \n",
    "        # Extract certification from the skills section if present\n",
    "        for line in lines:\n",
    "            if \"Certification:\" in line:\n",
    "                cert_text = line.split(\":\", 1)[1].strip()\n",
    "                if \"certifications\" not in self.extracted_data:\n",
    "                    self.extracted_data[\"certifications\"] = [{\"name\": cert_text}]\n",
    "                \n",
    "        return skills_dict\n",
    "    \n",
    "    def _extract_projects(self, text):\n",
    "        \"\"\"Extract project information\"\"\"\n",
    "        projects = []\n",
    "        \n",
    "        # Try to identify project entries\n",
    "        lines = text.split('\\n')\n",
    "        project_entries = []\n",
    "        current_project = None\n",
    "        current_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Check if this looks like a project header (often has | separator with technologies)\n",
    "            if \"|\" in line and len(line.split(\"|\")[0].strip()) < 30:\n",
    "                # This might be a new project\n",
    "                if current_project and current_lines:\n",
    "                    project_entries.append((current_project, current_lines))\n",
    "                    \n",
    "                current_project = line\n",
    "                current_lines = []\n",
    "            elif current_project:\n",
    "                current_lines.append(line)\n",
    "                \n",
    "        # Add the last project\n",
    "        if current_project and current_lines:\n",
    "            project_entries.append((current_project, current_lines))\n",
    "            \n",
    "        # Process each project entry\n",
    "        for project_header, project_lines in project_entries:\n",
    "            project = {}\n",
    "            \n",
    "            # Parse project name and technologies\n",
    "            if \"|\" in project_header:\n",
    "                parts = project_header.split(\"|\", 1)\n",
    "                project[\"name\"] = parts[0].strip()\n",
    "                \n",
    "                # Extract technologies\n",
    "                tech_text = parts[1].strip()\n",
    "                technologies = [tech.strip() for tech in tech_text.split(\",\") if tech.strip()]\n",
    "                if technologies:\n",
    "                    project[\"technologies\"] = technologies\n",
    "            else:\n",
    "                project[\"name\"] = project_header\n",
    "                \n",
    "            # Extract description (bullet points)\n",
    "            description = []\n",
    "            for line in project_lines:\n",
    "                if line.startswith(\"•\"):\n",
    "                    description.append(line.lstrip(\"• \").strip())\n",
    "                    \n",
    "            if description:\n",
    "                project[\"description\"] = description\n",
    "                \n",
    "            if project:\n",
    "                projects.append(project)\n",
    "                \n",
    "        return projects\n",
    "    \n",
    "    def _extract_certifications(self, text):\n",
    "        \"\"\"Extract certifications\"\"\"\n",
    "        certifications = []\n",
    "        \n",
    "        # Look for certification mentions\n",
    "        cert_pattern = r'Certification:?\\s*([^•\\n,]+)'\n",
    "        cert_matches = re.findall(cert_pattern, text, re.IGNORECASE)\n",
    "        \n",
    "        for cert_text in cert_matches:\n",
    "            cert_name = cert_text.strip()\n",
    "            if cert_name:\n",
    "                certifications.append({\"name\": cert_name})\n",
    "                \n",
    "        return certifications\n",
    "    \n",
    "    def _extract_languages(self, text):\n",
    "        \"\"\"Extract languages (not common but sometimes present)\"\"\"\n",
    "        languages = []\n",
    "        \n",
    "        language_pattern = r'(?:Languages|Language Proficiency):?\\s*([^•\\n]+)'\n",
    "        language_match = re.search(language_pattern, text, re.IGNORECASE)\n",
    "        \n",
    "        if language_match:\n",
    "            lang_text = language_match.group(1).strip()\n",
    "            langs = [lang.strip() for lang in re.split(r',|;', lang_text) if lang.strip()]\n",
    "            \n",
    "            for lang in langs:\n",
    "                if \":\" in lang:\n",
    "                    # Might include proficiency\n",
    "                    name, proficiency = lang.split(\":\", 1)\n",
    "                    languages.append({\n",
    "                        \"language\": name.strip(),\n",
    "                        \"proficiency\": proficiency.strip()\n",
    "                    })\n",
    "                else:\n",
    "                    languages.append({\"language\": lang})\n",
    "                    \n",
    "        return languages\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Parse resume PDF and convert to JSON')\n",
    "    parser.add_argument('input_file', help='Path to the resume PDF file')\n",
    "    parser.add_argument('--output', help='Output JSON file path')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create parser and parse resume\n",
    "    parser = ResumeParser()\n",
    "    \n",
    "    result = parser.parse_resume(args.input_file)\n",
    "    \n",
    "    if result:\n",
    "        # Format output\n",
    "        output_json = json.dumps(result, indent=2)\n",
    "        \n",
    "        # Determine output file path\n",
    "        if args.output:\n",
    "            output_file = args.output\n",
    "        else:\n",
    "            base_name = os.path.splitext(os.path.basename(args.input_file))[0]\n",
    "            output_file = f\"{base_name}_parsed.json\"\n",
    "        \n",
    "        # Write to file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(output_json)\n",
    "        \n",
    "        print(f\"Resume parsed successfully. Output saved to {output_file}\")\n",
    "        print(output_json)\n",
    "    else:\n",
    "        print(\"Failed to parse resume.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in d:\\bigdataintelligence\\test\\test\\.venv\\lib\\site-packages (0.11.6)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in d:\\bigdataintelligence\\test\\test\\.venv\\lib\\site-packages (from pdfplumber) (20250327)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in d:\\bigdataintelligence\\test\\test\\.venv\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: Pillow>=9.1 in d:\\bigdataintelligence\\test\\test\\.venv\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in d:\\bigdataintelligence\\test\\test\\.venv\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in d:\\bigdataintelligence\\test\\test\\.venv\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (44.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in d:\\bigdataintelligence\\test\\test\\.venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in d:\\bigdataintelligence\\test\\test\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'D:\\BigDataIntelligence\\test\\test\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3305487672.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    import [PyPDF2]\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import [PyPDF2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_hyperlinks_from_pdf(file_path):\n",
    "    \"\"\"Extract hyperlinks from PDF file using pdfplumber and PyPDF2\"\"\"\n",
    "    hyperlinks = []\n",
    "    \n",
    "    try:\n",
    "        # First, try with pdfplumber\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                # Extract annotations which may contain hyperlinks\n",
    "                if hasattr(page, 'annots') and page.annots:\n",
    "                    for annot in page.annots:\n",
    "                        if annot.get('Subtype') == 'Link' and annot.get('A', {}).get('URI'):\n",
    "                            uri = annot['A']['URI']\n",
    "                            rect = annot['Rect']\n",
    "                            hyperlinks.append({\n",
    "                                'url': uri,\n",
    "                                'page': page_num + 1,\n",
    "                                'rect': rect\n",
    "                            })\n",
    "        \n",
    "        # If no hyperlinks found with pdfplumber, try PyPDF2 as fallback\n",
    "        if not hyperlinks:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                \n",
    "                # Extract hyperlinks using PyPDF2\n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    if '/Annots' in page:\n",
    "                        for annot in page['/Annots']:\n",
    "                            annot_obj = annot.get_object()\n",
    "                            if annot_obj.get('/Subtype') == '/Link' and '/A' in annot_obj:\n",
    "                                a = annot_obj['/A']\n",
    "                                if '/URI' in a:\n",
    "                                    uri = a['/URI']\n",
    "                                    hyperlinks.append({\n",
    "                                        'url': uri,\n",
    "                                        'page': page_num + 1,\n",
    "                                        'rect': annot_obj.get('/Rect', [])\n",
    "                                    })\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting hyperlinks from PDF: {str(e)}\")\n",
    "    \n",
    "    return hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting hyperlinks from PDF: name 'pdfplumber' is not defined\n"
     ]
    }
   ],
   "source": [
    "res = _extract_hyperlinks_from_pdf(\"tanmay.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_hyperlinks_from_pdf(file_path):\n",
    "    \"\"\"Extract hyperlinks from PDF file using multiple libraries to get both link text and URLs\"\"\"\n",
    "    hyperlinks = []\n",
    "    \n",
    "    # Try PyMuPDF (fitz) first - best for getting text with links\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        for page_num, page in enumerate(doc):\n",
    "            link_list = page.get_links()\n",
    "            for link in link_list:\n",
    "                if link.get(\"uri\") and link.get(\"from\"):\n",
    "                    rect = link.get(\"from\")  # rectangle containing the link\n",
    "                    # Try to extract the text in this rectangle\n",
    "                    words = page.get_text(\"words\", clip=rect)\n",
    "                    link_text = \" \".join([word[4] for word in words]) if words else \"\"\n",
    "                    \n",
    "                    hyperlinks.append({\n",
    "                        'text': link_text,\n",
    "                        'url': link.get(\"uri\"),\n",
    "                        'page': page_num + 1,\n",
    "                        'rect': rect\n",
    "                    })\n",
    "        doc.close()\n",
    "    except Exception as e:\n",
    "        print(f\"PyMuPDF extraction error: {str(e)}\")\n",
    "    \n",
    "    # If PyMuPDF didn't find links, try pdfplumber\n",
    "    if not hyperlinks:\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages):\n",
    "                    if hasattr(page, 'annots') and page.annots:\n",
    "                        for annot in page.annots:\n",
    "                            if annot.get('Subtype') == 'Link' and annot.get('A', {}).get('URI'):\n",
    "                                uri = annot['A']['URI']\n",
    "                                rect = annot['Rect']\n",
    "                                \n",
    "                                # Try to get text near this rectangle\n",
    "                                words = page.extract_words(x_tolerance=3, y_tolerance=3)\n",
    "                                nearby_words = [w for w in words if is_point_in_rect((w['x0'], w['top']), rect, margin=5)]\n",
    "                                link_text = \" \".join([w['text'] for w in nearby_words]) if nearby_words else \"\"\n",
    "                                \n",
    "                                hyperlinks.append({\n",
    "                                    'text': link_text,\n",
    "                                    'url': uri,\n",
    "                                    'page': page_num + 1,\n",
    "                                    'rect': rect\n",
    "                                })\n",
    "        except Exception as e:\n",
    "            print(f\"Pdfplumber extraction error: {str(e)}\")\n",
    "    \n",
    "    # Last resort: try PyPDF2\n",
    "    if not hyperlinks:\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    if '/Annots' in page:\n",
    "                        for annot in page['/Annots']:\n",
    "                            annot_obj = annot.get_object()\n",
    "                            if annot_obj.get('/Subtype') == '/Link' and '/A' in annot_obj:\n",
    "                                a = annot_obj['/A']\n",
    "                                if '/URI' in a:\n",
    "                                    uri = a['/URI']\n",
    "                                    rect = annot_obj.get('/Rect', [])\n",
    "                                    hyperlinks.append({\n",
    "                                        'text': '',  # PyPDF2 doesn't easily give us the text\n",
    "                                        'url': uri,\n",
    "                                        'page': page_num + 1,\n",
    "                                        'rect': rect\n",
    "                                    })\n",
    "        except Exception as e:\n",
    "            print(f\"PyPDF2 extraction error: {str(e)}\")\n",
    "    \n",
    "    return hyperlinks\n",
    "\n",
    "def is_point_in_rect(point, rect, margin=0):\n",
    "    \"\"\"Check if a point is inside or near a rectangle\"\"\"\n",
    "    x, y = point\n",
    "    x0, y0, x1, y1 = rect\n",
    "    return (x0 - margin <= x <= x1 + margin) and (y0 - margin <= y <= y1 + margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'pawar.ta@northeastern.edu', 'url': 'mailto:pawar.ta@northeastern.edu'}, {'text': 'LinkedIn', 'url': 'https://www.linkedin.com/in/tanmay-pawar-73a9201ba/'}, {'text': 'GitHub', 'url': 'https://github.com/Tanmay6461/'}, {'text': 'Findata', 'url': 'https://github.com/bigdata-org/financial_data_extraction.git'}, {'text': 'Pytract', 'url': 'https://github.com/Tanmay6461/PyTract.git'}, {'text': 'MediSense', 'url': 'https://github.com/Tanmay6461/Data-Extraction-and-Retrieval.git'}]\n"
     ]
    }
   ],
   "source": [
    "print(extract_hyperlinks_from_pdf(\"tanmay.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_hyperlinks_from_pdf(file_path):\n",
    "    \"\"\"Extract hyperlinks from PDF file using PyMuPDF to get both link text and URLs\"\"\"\n",
    "    hyperlinks = []\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        for page_num, page in enumerate(doc):\n",
    "            link_list = page.get_links()\n",
    "            for link in link_list:\n",
    "                if link.get(\"uri\"):\n",
    "                    # Get the rectangle containing the link\n",
    "                    rect = link.get(\"from\")\n",
    "                    \n",
    "                    # Extract the text in this rectangle if available\n",
    "                    link_text = \"\"\n",
    "                    if rect:\n",
    "                        words = page.get_text(\"words\", clip=rect)\n",
    "                        link_text = \" \".join([word[4] for word in words]) if words else \"\"\n",
    "                    \n",
    "                    hyperlinks.append({\n",
    "                        'text': link_text,\n",
    "                        'url': link.get(\"uri\")\n",
    "                    })\n",
    "        doc.close()\n",
    "    except Exception as e:\n",
    "        print(f\"PyMuPDF extraction error: {str(e)}\")\n",
    "    \n",
    "    return hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
